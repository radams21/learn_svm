}
summary_complete$kernel <- kernel_types[i]
all_tuned_kernels <- rbind(all_tuned_kernels, summary_complete)
# display kernel name, performance, and parameters to the screen
cat(paste(kernel_types[i],"\n"))
cat(paste("\t",tune_output$best.performance, "\n"))
cat(paste("\t",tune_output$best.parameters, "\n"))
## plot tuning parameters for a kernel
plot_model_tuning_param_performance(tune_output, kernel_types[i])
# determine which of the models is the best (lowest error rate)
if(best_performance > tune_output$best.performance) {
cat("\tnew best model\n")
best_params <- tune_output
best_kernel <- kernel_types[i]
best_performance <- tune_output$best.performance
}
}
compare_kernels_plot <- plot_all_tuned_kernels(all_tuned_kernels)
compare_kernels_plot
# return the kernel name and parameters for the best model
return (as.list(c(best_kernel, best_params, compare_kernels_plot)))
}
## function to tune the parameters for a specific kernel
tune_model <- function(train_set,
kernel_type = "radial",
max_gamma = -2, min_gamma = -6,
max_cost = 3, min_cost = -6,
max_degree = 5, min_degree = 2) {
if(kernel_type == "linear") {
tuned <- tune.svm(survival_days ~ ., data = train_set,
kernel = kernel_type,
cost = 10^(min_cost:max_cost))
} else if(kernel_type == "polynomial") {
tuned <- tune.svm(survival_days ~ ., data = train_set,
kernel = kernel_type,
gamma = 10^(min_gamma:max_gamma),
cost = 10^(min_cost:max_cost),
degree = (min_degree:max_degree))
} else {
tuned <- tune.svm(survival_days ~ ., data = train_set,
kernel = kernel_type,
gamma = 10^(min_gamma:max_gamma),
cost = 10^(min_cost:max_cost))
}
}
## Summarizes data.
## Gives count, mean, standard deviation, standard error of the mean, and confidence interval (default 95%).
##   data: a data frame.
##   measurevar: the name of a column that contains the variable to be summariezed
##   groupvars: a vector containing names of columns that contain grouping variables
##   na.rm: a boolean that indicates whether to ignore NA's
##   conf.interval: the percent range of the confidence interval (default is 95%)
## function to generate a continuous value for the dependent variable (surivival_days)
## based on a linear transformation of 10-20 columns (genes)
generate_survival_info <- function(table1, max_survival, num_patients) {
table1$survival_days <- 0
method_opt = 1
if(method_opt == 1) {
## choose 10-20 variables (columns) to USE
num_cols <- sample(10:20, 1)
cols_to_use <- sample(1:(ncol(table1)-1), num_cols, replace=T)
## select a random coefficient for each column to change
random_coefs <- sample((-50):50, num_cols, replace=T)
## apply linear transformations to selected columns
## save the unmodified columns and transformed data in a new table (table2)
for (iCol in 1:num_cols) {
table1$survival_days <- table1$survival_days + table1[, cols_to_use[iCol]] * random_coefs[iCol]
}
table1$survival_days <- table1$survival_days + rnorm(nrow(table1))
## choose 10-30 variables (columns) to USE
aFewMoreVars <- sample(10:30, 1)
moreColsToUse <- sample(1:(ncol(table1)-1), aFewMoreVars)
newCol <- 1
for (iCol in moreColsToUse) {
newCol <- newCol * table1[, iCol] * rnorm(1)
}
aFewMoreVars <- sample(10:30, 1)
moreColsToUse <- sample(1:(ncol(table1)-1), aFewMoreVars)
newCol <- 0
for (iCol in moreColsToUse) {
newCol <- newCol + table1[, iCol] * rnorm(1)
}
table1$survival_days <- (table1$survival_days + exp(newCol)) %% 1000
} else {
## choose 10-20 variables (columns) to USE
num_cols <- sample(10:20, 1)
cols_to_use <- sample(1:(ncol(table1)-1), num_cols, replace=T)
## select a random coefficient for each column to change
random_coefs <- sample((-50):50, num_cols, replace=T)
## apply linear transformations to selected columns
## save the unmodified columns and transformed data in a new table (table2)
for (iCol in 1:num_cols) {
table1$survival_days <- table1$survival_days + table1[, cols_to_use[iCol]] * random_coefs[iCol]
}
table1$survival_days <- table1$survival_days + rnorm(nrow(table1))
}
table1
}
## run model
run_model <- function(train_set, kernel, tuned_cost, tuned_gamma=0, cross=0) {
svm.model <- svm(survival_days ~ ., data = train_set, kernel = best_tuned_kernel,
cost = tuned_cost, gamma = tuned_gamma, cross = cross)
}
## run prediction
predict_model<- function(svm.model, test_set) {
svm.pred <- predict(svm.model, test_set[,1:(ncol(test_set)-1)])
}
## function to read in data
read_data_from_file <- function(data_file = "ncbi.data", max_genes = -1) {
## read in data from file
table1 <- read.table(data_file, header=F)
## select a subset of genes to work with
if(max_genes > 1 && max_genes <= nrow(table1)) {
table1 <- table1[1:max_genes,]
}
table1
}
## function to generate a continuous value for the dependent variable (surivival_days)
## based on a linear transformation of 10-20 columns (genes)
generate_survival_info <- function(table1, max_survival, num_patients) {
table1$survival_days <- 0
method_opt = 1
if(method_opt == 1) {
## choose 10-20 variables (columns) to USE
num_cols <- sample(10:20, 1)
cols_to_use <- sample(1:(ncol(table1)-1), num_cols, replace=T)
## select a random coefficient for each column to change
random_coefs <- sample((-50):50, num_cols, replace=T)
## apply linear transformations to selected columns
## save the unmodified columns and transformed data in a new table (table2)
for (iCol in 1:num_cols) {
table1$survival_days <- table1$survival_days + table1[, cols_to_use[iCol]] * random_coefs[iCol]
}
table1$survival_days <- table1$survival_days + rnorm(nrow(table1))
## choose 10-30 variables (columns) to USE
aFewMoreVars <- sample(10:30, 1)
moreColsToUse <- sample(1:(ncol(table1)-1), aFewMoreVars)
newCol <- 1
for (iCol in moreColsToUse) {
newCol <- newCol * table1[, iCol] * rnorm(1)
}
aFewMoreVars <- sample(10:30, 1)
moreColsToUse <- sample(1:(ncol(table1)-1), aFewMoreVars)
newCol <- 0
for (iCol in moreColsToUse) {
newCol <- newCol + table1[, iCol] * rnorm(1)
}
table1$survival_days <- (table1$survival_days + exp(newCol)) %% 1000
} else {
## choose 10-20 variables (columns) to USE
num_cols <- sample(10:20, 1)
cols_to_use <- sample(1:(ncol(table1)-1), num_cols, replace=T)
## select a random coefficient for each column to change
random_coefs <- sample((-50):50, num_cols, replace=T)
## apply linear transformations to selected columns
## save the unmodified columns and transformed data in a new table (table2)
for (iCol in 1:num_cols) {
table1$survival_days <- table1$survival_days + table1[, cols_to_use[iCol]] * random_coefs[iCol]
}
table1$survival_days <- table1$survival_days + rnorm(nrow(table1))
}
table1
}
## function to find the best model type and tune its parameters
## returns the model's kernel name and parameters with the best performance (lowest error)
find_best_params<- function(train_set, max_gamma, min_gamma, max_cost, min_cost, max_degree, min_degree) {
#kernel_types <- c("radial", "linear", "polynomial", "sigmoid")
kernel_types <- c("radial", "linear")
best_performance <- 10^15
best_kernel <- kernel_types[1]
all_tuned_kernels <- as.data.frame(rbind(rep(0, 2), rep(0, 2)))
colnames(all_tuned_kernels) <- c("ParamValue", "Error")
all_tuned_kernels <- summarySE(all_tuned_kernels, measurevar = "Error", groupvars=c("ParamValue"), na.rm=T)
all_tuned_kernels$Parameter <- NA
all_tuned_kernels$kernel <- NA
# iterate through each kernel type
for(i in 1:length(kernel_types)) {
all_tune_param_values <- data.frame()
for(j in 1:10) {
# tune parameters for the specific kernel type
tune_output <- tune_model(train_set, kernel_types[i],
max_gamma, min_gamma,
max_cost, min_cost)
all_tune_param_values <- rbind(all_tune_param_values, tune_output$performances)
}
cost_table <- unique(all_tune_param_values[,c('cost','error')])
colnames(cost_table) <- c("Cost", "Error")
cost_summary <- summarySE(cost_table, measurevar="Error", groupvars = c("Cost"), na.rm=T)
cost_summary$Parameter <- "Cost"
colnames(cost_summary)[1] <- "ParamValue"
cost_summary <- na.omit(cost_summary)
if(kernel_types[i] != "linear") {
gamma_table <- unique(all_tune_param_values[,c('gamma','error')])
colnames(gamma_table) <- c("Gamma", "Error")
gamma_summary <- summarySE(gamma_table, measurevar="Error", groupvars = c("Gamma"), na.rm=T)
gamma_summary$Parameter <- "Gamma"
colnames(gamma_summary)[1] <- "ParamValue"
summary_complete <- rbind(cost_summary, gamma_summary)
if(kernel_types[i] == "polynomial") {
degree_table <- unique(all_tune_param_values[,c('degree','error')])
colnames(degree_table) <- c("Degree", "Error")
degree_summary <- summarySE(degree_table, measurevar="Error", groupvars = c("Degree"), na.rm=T)
degree_summary$Parameter <- "Degree"
colnames(degree_summary)[1] <- "ParamValue"
summary_complete <- rbind(cost_summary, degree_summary)
}
} else {
summary_complete <- cost_summary
}
summary_complete$kernel <- kernel_types[i]
all_tuned_kernels <- rbind(all_tuned_kernels, summary_complete)
# display kernel name, performance, and parameters to the screen
cat(paste(kernel_types[i],"\n"))
cat(paste("\t",tune_output$best.performance, "\n"))
cat(paste("\t",tune_output$best.parameters, "\n"))
## plot tuning parameters for a kernel
plot_model_tuning_param_performance(tune_output, kernel_types[i])
# determine which of the models is the best (lowest error rate)
if(best_performance > tune_output$best.performance) {
cat("\tnew best model\n")
best_params <- tune_output
best_kernel <- kernel_types[i]
best_performance <- tune_output$best.performance
}
}
compare_kernels_plot <- plot_all_tuned_kernels(all_tuned_kernels)
compare_kernels_plot
# return the kernel name and parameters for the best model
return (as.list(c(best_kernel, best_params, compare_kernels_plot)))
}
## function to tune the parameters for a specific kernel
tune_model <- function(train_set,
kernel_type = "radial",
max_gamma = -2, min_gamma = -6,
max_cost = 3, min_cost = -6,
max_degree = 5, min_degree = 2) {
if(kernel_type == "linear") {
tuned <- tune.svm(survival_days ~ ., data = train_set,
kernel = kernel_type,
cost = 10^(min_cost:max_cost))
} else if(kernel_type == "polynomial") {
tuned <- tune.svm(survival_days ~ ., data = train_set,
kernel = kernel_type,
gamma = 10^(min_gamma:max_gamma),
cost = 10^(min_cost:max_cost),
degree = (min_degree:max_degree))
} else {
tuned <- tune.svm(survival_days ~ ., data = train_set,
kernel = kernel_type,
gamma = 10^(min_gamma:max_gamma),
cost = 10^(min_cost:max_cost))
}
}
## Summarizes data.
## Gives count, mean, standard deviation, standard error of the mean, and confidence interval (default 95%).
##   data: a data frame.
##   measurevar: the name of a column that contains the variable to be summariezed
##   groupvars: a vector containing names of columns that contain grouping variables
##   na.rm: a boolean that indicates whether to ignore NA's
##   conf.interval: the percent range of the confidence interval (default is 95%)
## function to generate 2 plots (boxplot and histogram)
## depicts residuals of the svm model's fitted data compared to the training set's true survival outcomes
plot_model_residuals <- function(best_tuned_kernel, svm.model) {
xlab <- paste("Model Residuals from", best_tuned_kernel, "kernel on a dataset of", max_genes, "genes")
g <- ggplot() + geom_boxplot(aes(x=xlab, y=svm.model$residuals))
filename <- generate_filename(dir="nonlinear_output",best_tuned_kernel,"model_residuals",".pdf")
ggsave(filename, g)
g <- ggplot() + geom_histogram(aes(svm.model$residuals))
filename <- generate_filename(dir="nonlinear_output",best_tuned_kernel,"model_residuals_hist",".pdf")
ggsave(filename, g)
}
## function to generate linear regression plot of predicted vs true survival outcomes
## label includes an adjusted R2 value as an approximator for the accuracy of the model
plot_prediction_vs_true_r2 <- function(best_tuned_params, best_tuned_kernel, svm.pred, test_set) {
filename <- generate_filename(dir="nonlinear_output",best_tuned_kernel,"predict_corr_r2",".pdf")
pdf(filename)
params_with_labels <- as.vector(sapply(names(best_tuned_params),
function(i)
paste(i, ":", best_tuned_params[i][[1]])))
plot(test_set$survival_days, svm.pred,
main = paste(best_tuned_kernel, params_with_labels, sep=", "),
xlab = "True Survival Days", ylab = "Predicted Survival Days")
par(new=T)
fit2 <- lm(svm.pred ~ test_set$survival_days)
abline(fit2)
legend("topleft", bty="n",
legend=paste("R2 is", format(summary(fit2)$adj.r.squared, digits=4)))
dev.off()
}
## function to generate a plot of a model's performance (x-axis) vs a range of tuning parameters (y-axis)
## for linear models, the y-axis is the log10 of the cost parameter
## for non-linear models, there is a second y-axis with the log10 of the gamma parameter
plot_model_tuning_param_performance <- function(tune_output, best_tuned_kernel) {
x <- tune_output$performances$error
y1 <- log(tune_output$performances$cost, 10)
filename <- generate_filename(dir="nonlinear_output",best_tuned_kernel,"model_tuning_param_perf",".pdf")
pdf(filename)
plot(x, y1, col="red",
main = "Tuning Model's Parameters to Reduce Error",
xlab = "Error", ylab = "log10(Cost)",
xlim=c(min(x)-10, max(x)+10))
if(best_tuned_kernel != "linear") {
y2 <- log(tune_output$performances$gamma, 10)
par(new=TRUE)
plot(x, y2, col="blue", pch=2,
xaxt="n", yaxt="n",
ylab="", xlab="",
xlim=c(min(x)-10, max(x)+10))
axis(4)
mtext("log10(Gamma)",side=4,line=3)
legend("topleft",col=c("red","blue"),lty=1, legend=c("Cost","Gamma"), pch=1,
cex=0.7)
}
dev.off()
}
plot_all_tuned_kernels <- function(all_tuned_kernels) {
all_tuned_kernels <- na.omit(all_tuned_kernels)
g<- ggplot(all_tuned_kernels, aes(x=log10(ParamValue), y=Error, colour=Parameter)) +
geom_errorbar(aes(ymin=Error-se, ymax=Error+se), width=.1) +
geom_line() +
geom_point() + facet_grid(.~kernel) +
ggtitle("Tuning Parameters For Each Kernel Method")
filename <- generate_filename(dir="nonlinear_output","all_kernels","tuning_param_perf",".pdf")
ggsave(filename, g)
g
}
## function to generate both model residual plots and predictor regression plots
## this is intended to be called after the parameters for a specific kernel type have been tuned
generate_model_pred_plots <- function(best_tuned_params, best_tuned_kernel, svm.model, test_set, svm.pred) {
## plot model residuals of train set
plot_model_residuals(best_tuned_kernel, svm.model)
## plot prediction outcomes and true values
plot_prediction_vs_true_r2(best_tuned_params, best_tuned_kernel, svm.pred, test_set)
}
## run model
run_model <- function(train_set, kernel, tuned_cost, tuned_gamma=0, cross=0) {
svm.model <- svm(survival_days ~ ., data = train_set, kernel = best_tuned_kernel,
cost = tuned_cost, gamma = tuned_gamma, cross = cross)
}
## run prediction
predict_model<- function(svm.model, test_set) {
svm.pred <- predict(svm.model, test_set[,1:(ncol(test_set)-1)])
}
generate_filename <- function(dir="", kernel,  desc_name, extension) {
filename = ""
if(dir != "") {
filename = paste(dir,"/", sep="")
}
filename = paste(filename, max_genes, "genes_", kernel,"_", desc_name, extension, sep="")
}
## function to set working directory
set_workdir <- function (my_workdir = ".") {
## set working directory
if(!file.exists(my_workdir)) {
stop(paste("Working directory is not valid or does not exist:\n'",
my_workdir, "'", sep=""))
}
setwd(my_workdir)
}
#####################
# MAIN FUNCTIONS
#####################
set_workdir()
max_genes = 50
table1 <- read_data_from_file(max_genes=max_genes)
## table format is genes (rows) x patients (columns)
## initially, no survival data is available
num_patients <- ncol(table1)
num_genes <- nrow(table1)
## designate Patient Ids ("Patient1", "Patient2", etc)
colnames(table1) <- lapply(seq(1,num_patients),
function(i) paste("Patient", i, sep=""))
## reformat the table to contain patients (rows) x genes (columns)
matrix1 <- t(as.matrix(table1))
table1 <- as.data.frame(matrix1)
## generate and add survival_days column to end of table
max_survival = 50
table2 <- generate_survival_info(table1, max_survival, num_patients)
## split data into a train and test set
index <- 1:nrow(table2)
test_index <- sample(index, trunc(length(index)/3))
test_set <- table2[test_index,]
train_set <- table2[-test_index,]
## tune model
max_gamma <- 1
min_gamma <- -5
max_cost <- 1
min_cost <- -5
min_degree <- 2
max_degree <- 5
all_tune_output <- find_best_params(train_set,
max_gamma, min_gamma,
max_cost, min_cost,
max_degree, min_degree)
best_tuned_kernel <- all_tune_output[[1]]
best_tuned_params <- all_tune_output[[2]]
compare_kernels_plot <- all_tune_output[[3]]
tuned_cost <- best_tuned_params["cost"][[1]]
tuned_gamma <- 0
if(best_tuned_kernel != "linear") {
tuned_gamma <- best_tuned_params["gamma"][[1]]
## run model
svm.model <- run_model(train_set, kernel, tuned_cost, tuned_gamma)
} else {
## run model
svm.model <- run_model(train_set, kernel, tuned_cost)
}
## run prediction
svm.pred <- predict_model(svm.model, test_set)
## compare prediction and real values
comparison <- cbind(svm.pred, test_set$survival_days)
View(comparison)
best_tuned_kernel
svm.model
plot_model_residuals(best_tuned_kernel, svm.model)
plot_model_residuals(best_tuned_kernel, svm.model)
svm.model$residuals
xlab <- paste("Model Residuals from", best_tuned_kernel, "kernel on a dataset of", max_genes, "genes")
g <- ggplot() + geom_boxplot(aes(x=xlab, y=svm.model$residuals))
filename <- generate_filename(dir="nonlinear_output",best_tuned_kernel,"model_residuals",".pdf")
ggsave(filename, g)
g <- ggplot() + geom_histogram(aes(svm.model$residuals))
filename <- generate_filename(dir="nonlinear_output",best_tuned_kernel,"model_residuals_hist",".pdf")
ggsave(filename, g)
g <- ggplot() + geom_boxplot(aes(y=svm.model$residuals))
filename <- generate_filename(dir="nonlinear_output",best_tuned_kernel,"model_residuals",".pdf")
ggsave(filename, g)
g <- ggplot() + geom_boxplot(aes(x=rep("x",3),y=svm.model$residuals))
filename <- generate_filename(dir="nonlinear_output",best_tuned_kernel,"model_residuals",".pdf")
ggsave(filename, g)
g <- ggplot() + geom_boxplot(aes(x="x",y=svm.model$residuals))
filename <- generate_filename(dir="nonlinear_output",best_tuned_kernel,"model_residuals",".pdf")
ggsave(filename, g)
g <- ggplot() + geom_histogram(aes(svm.model$residuals))
filename <- generate_filename(dir="nonlinear_output",best_tuned_kernel,"model_residuals_hist",".pdf")
ggsave(filename, g)
?ggsave
## choose 10-30 additional variables (columns) to USE
aFewMoreVars <- sample(10:30, 1)
moreColsToUse <- sample(1:(ncol(table1)-1), aFewMoreVars)
newCol <- 1
for (iCol in moreColsToUse) {
newCol <- newCol * table1[, iCol] * rnorm(1)
}
newCol
aFewMoreVars <- sample(10:30, 1)
moreColsToUse <- sample(1:(ncol(table1)-1), aFewMoreVars)
newCol <- 0
aFewMoreVars <- sample(10:30, 1)
moreColsToUse <- sample(1:(ncol(table1)-1), aFewMoreVars)
newCol <- 0
for (iCol in moreColsToUse) {
newCol <- newCol + table1[, iCol] * rnorm(1)
}
newCol
exp(3)
table1$survival_days <- 0
table1$survival_days <- (table1$survival_days + exp(newCol)) %% 1000
hist(table1$survival_days)
aFewMoreVars <- sample(10:30, 1)
moreColsToUse <- sample(1:(ncol(table1)-1), aFewMoreVars)
newCol <- 1
for (iCol in moreColsToUse) {
newCol <- newCol * table1[, iCol] * rnorm(1)
}
table1$survival_days <- (table1$survival_days + exp(newCol)) %% 1000
hist(table1$survival_days)
aFewMoreVars <- sample(10:30, 1)
moreColsToUse <- sample(1:(ncol(table1)-1), aFewMoreVars)
newCol <- 1
for (iCol in moreColsToUse) {
newCol <- newCol * table1[, iCol] * rnorm(1)
}
table1$survival_days <- table1$survival_days + exp(newCol)
hist(table1$survival_days)
newCol
rnorm(i)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
rnorm(1)
exp(-2.3)
table1$survival_days <- 0
aFewMoreVars <- sample(10:30, 1)
moreColsToUse <- sample(1:(ncol(table1)-1), aFewMoreVars)
newCol <- 0
for (iCol in moreColsToUse) {
newCol <- newCol + table1[, iCol] * rnorm(1)
}
table1$survival_days <- table1$survival_days + exp(newCol)
hist(table1$survival_days)
table1$survival_days <- 0
aFewMoreVars <- sample(10:30, 1)
moreColsToUse <- sample(1:(ncol(table1)-1), aFewMoreVars)
newCol <- 0
for (iCol in moreColsToUse) {
newCol <- newCol + table1[, iCol] * rnorm(1)
}
table1$survival_days <- table1$survival_days + exp(newCol)
hist(table1$survival_days)
install.packages("png")
?pdflatex
Sys.getenv("PATH")
Sys.which("pdflatex")
